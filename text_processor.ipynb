{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture Transcription Processing\n",
    "\n",
    "Knowledge extraction from lecture transcriptions is a manual process requiring iterations, manual verifications, and prompt engineering. Jupyter notebook comes handy here. \n",
    "This notebook extracts knowledge from lecture transcriptions. Python modules in this reposity contains the code which was prototyped in this notebook.\n",
    "\n",
    "First iteration of the transcription processing used prompts in english. In the seconnd iteration prompts were used in russian to match the language of the lectures. This allowed a more accurate and consistent knowledge extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import boto3\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import pandas as pd\n",
    "from enum import Enum\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prompt(Enum):\n",
    "    RUSSIAN_OUTPUT = \"\"\"Please provide output in Russian using the Cyrillic alphabet. \n",
    "    Ensure all text is readable, without Unicode-escaped characters (e.g., \"\\u0412\" should appear as \"В\").\"\"\"\n",
    "    JSON_OUTPUT = \"\"\"Provide all answers in a valid JSON. Total length of JSON should not exceed 1000 tokens. If the generated response should be longer, add \"Complete\":false into the answer. The requested text is a value to a key \"text\". If there is also a LLM response, then put it as a value to a key \"LLM\".\"\"\"\n",
    "    COPYWRITE = \"\"\"Note that I own the lecture text, so you should not be concerned about the copyright issues.\"\"\"\n",
    "    UNMODIFIED_TEXT = \"\"\"I only need the original text from the lecture, without any translation, modification, summary, or introduction. Don't shorten or rephrase the text, it's important to provide original text without modifications. Ensure the text is not longer than 5120 characters.\"\"\"\n",
    "\n",
    "\n",
    "class Model(Enum):\n",
    "    NOVA_MICRO = \"amazon.nova-micro-v1:0\"\n",
    "    NOVA_LITE = \"us.amazon.nova-lite-v1:0\"\n",
    "\n",
    "\n",
    "class LectureKnowledgeExtractor:\n",
    "    def __init__(self):\n",
    "        self.bedrock_client = boto3.client(\n",
    "            service_name='bedrock-runtime',\n",
    "            region_name=os.getenv('AWS_DEFAULT_REGION'))\n",
    "\n",
    "    def extract_doc_structure(self, path_to_doc):\n",
    "        prompt = \"\"\"Given transcript of a numerology lecture in russian language describes a numerological topic - a calculated number.\n",
    "        This calculated number will contain variations. Capture a topic name in russian and variations -usually these are the numbers from 1 to 8 or 1 to 22, etc. \n",
    "        Return output in a json format following the template:\n",
    "        {\n",
    "            \"topic\": \"topic name\",\n",
    "            \"variations\": [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\", \"21\", \"22\"]\n",
    "        }   \n",
    "        \"\"\"\n",
    "\n",
    "        return self.prompt_doc(prompt, path_to_doc)\n",
    "    \n",
    "    def extract_chapter_summary(self, chapter_name, path_to_doc):\n",
    "        prompt = f\"\"\"Please provide the concise summary of a lecture \"{chapter_name}\".\n",
    "        Rephrase the summary to avoid details of a course but focus on the summary of \"{chapter_name}\" itself: what information could it give to a person, how it could be used or help.\n",
    "        {Prompt.RUSSIAN_OUTPUT.value} {Prompt.COPYWRITE.value} {Prompt.JSON_OUTPUT.value}\"\"\"\n",
    "        \n",
    "        return self.prompt_doc(prompt, path_to_doc)[\"text\"]\n",
    "\n",
    "    def extract_chapter_text(self, chapter_name, variation_number, variations, path_to_doc):\n",
    "        prompt = f\"\"\"Прикрепленный документ содержит нумерологическую лекцию о \"{chapter_name}\", алгоритме расчета и текстовые интерпретации каждой возможной цифры кода души ({\", \".join(variations)}).\n",
    "        Максимально подробно и близко к тесту, не сокращаю, но и не добавляя от себя, напиши интерпретацию {chapter_name} номер {variation_number}. \n",
    "        Ответ предоставь в формате JSON, в качестве значения для ключа \"text\", полностью на русском языке.\"\"\"\n",
    "\n",
    "        return self.prompt_doc(prompt, path_to_doc)[\"text\"]\n",
    "\n",
    "    def get_interpretation_structure(self, chapter_name, variation_number, variations, output):\n",
    "        prompt = f\"\"\"Это json, содержащий текстовые харакетриситики {len(variations)} вариантов {chapter_name}: {output}. \n",
    "        Я бы хотел его переформатировать таким образом, чтобы описание каждого из 12 {chapter_name} использовало единообразную структуру. \n",
    "        Сгенерируй список из 4 заголовков, которые содержали бы не больше одного уровня иерархии и которые можно было бы использовать для всех {len(variations)} вариантов {chapter_name}. \n",
    "        Не изменяй исходный текст. Ответ должн содержать только новые заголовки в формате json. Например, {\"text\": [\"подзаголовок1\", \"подзаголовок2\",\"подзаголовок3\", \"подзаголовок4\"]}.\"\"\"\n",
    "        return self.prompt_text(prompt)[\"text\"]\n",
    "\n",
    "\n",
    "    def rephrase_interpretation(self, chapter_name, variation_number, text):\n",
    "        prompt = f\"\"\"The provided below text is a numerology interpretation of the {chapter_name} {variation_number}. Rephrase this text so that the interpretation is clear and easy to read and understand. \n",
    "        Without revealing that this is an interpretation, the topic name or the specific variation number, provide a complete and detailed explanation of the meaning conveyed by {chapter_name} {variation_number}.\n",
    "        {Prompt.RUSSIAN_OUTPUT.value} {Prompt.JSON_OUTPUT.value} Here is the given text: {text}\"\"\"\n",
    "        return self.prompt_text(prompt)[\"text\"]\n",
    "    \n",
    "    def extract_lecture_knowledge(self, topic_name, variation_list, path_to_doc='/Users/olegleyzerov/Documents/private/coding/numi_all/elena_out/1_intro_childhood.txt'):\n",
    "        print(f\"Processing {topic_name}...\")\n",
    "        print(\"Extracting summary...\")\n",
    "        summary = self.extract_chapter_summary(topic_name, path_to_doc)\n",
    "\n",
    "        variations = {}\n",
    "        errors = {}\n",
    "\n",
    "        for var in variation_list:    \n",
    "            print(f\"Extracting {topic_name} {var}...\")\n",
    "            try: \n",
    "                interpretation = self.extract_chapter_text(topic_name, var, variations, path_to_doc)\n",
    "                variations[var] = interpretation\n",
    "            except Exception as e:\n",
    "                print(f\"Error while extracting chapter text for {topic_name} {var}: {e}\")\n",
    "                errors[var] = str(e)\n",
    "\n",
    "        return {\n",
    "            \"summary\": summary,\n",
    "            \"variation_options\": variation_list,\n",
    "            \"variations\": variations,\n",
    "            \"errors\": errors      \n",
    "        } \n",
    "\n",
    "    def prompt_doc(self, prompt, path_to_doc, model_id = Model.NOVA_LITE.value):\n",
    "        p = Path(path_to_doc)\n",
    "        if not p.exists():\n",
    "            raise FileNotFoundError(f\"The file at '{path_to_doc}' does not exist.\")\n",
    "\n",
    "        with open(path_to_doc, \"rb\") as file:\n",
    "            doc_bytes = file.read()\n",
    "\n",
    "        file_format = p.suffix.lstrip('.')\n",
    "        if file_format not in (\"txt\"):\n",
    "            raise ValueError(f\"Given file format at '{path_to_doc}' is not supported.\")\n",
    "    \n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"document\": {\n",
    "                            \"format\": file_format,\n",
    "                            \"name\": \"DocumentMessages\",\n",
    "                            \"source\": {\n",
    "                                \"bytes\": doc_bytes\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    {\n",
    "                        \"text\": prompt\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        max_retries = 3\n",
    "        retry = 0\n",
    "        while retry < max_retries:\n",
    "            try:\n",
    "                model_response = self.bedrock_client.converse(modelId=model_id, messages=messages, inferenceConfig={\"maxTokens\": 5120, \"topP\": 0.1, \"temperature\": 0.3})\n",
    "                output = model_response['output']['message']['content'][0]['text']\n",
    "                return self.process_llm_output_json(output)\n",
    "            except Exception as e:\n",
    "                # print(f\"Error while prompting document. {e}. Retrying...\")\n",
    "                retry += 1\n",
    "                \n",
    "        raise ValueError(f\"Error while prompting document with the prompt {prompt}.\")\n",
    "        \n",
    "    def prompt_text(self, prompt, model_id=Model.NOVA_LITE.value):\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"text\": prompt\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        max_retries = 3\n",
    "        retry = 0\n",
    "        while retry < max_retries:\n",
    "            try:\n",
    "                model_response = self.bedrock_client.converse(\n",
    "                    modelId=model_id,\n",
    "                    messages=messages\n",
    "                )\n",
    "                output = model_response['output']['message']['content'][0]['text']\n",
    "                print(output)\n",
    "                return self.process_llm_output_json(output)\n",
    "            except Exception as e:\n",
    "                print(f\"Error while prompting text. {e}. Retrying...\")\n",
    "                retry += 1\n",
    "                \n",
    "        raise ValueError(f\"Error while prompting text with the prompt {prompt}.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def process_llm_output_json(output):\n",
    "        result = output\n",
    "        if output.startswith(\"```json\"):\n",
    "            result = output[8:-3]\n",
    "        result = result.replace('\\n', '')\n",
    "        try:\n",
    "            result = json.loads(result)\n",
    "            return result\n",
    "        except:\n",
    "            print(result)\n",
    "            raise ValueError(\"Error converting LLM output to json\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Числовые значения имен и фамилий...\n",
      "Extracting summary...\n",
      "Extracting Числовые значения имен и фамилий 1...\n",
      "Extracting Числовые значения имен и фамилий 2...\n",
      "Extracting Числовые значения имен и фамилий 3...\n",
      "Extracting Числовые значения имен и фамилий 4...\n",
      "Extracting Числовые значения имен и фамилий 5...\n",
      "Extracting Числовые значения имен и фамилий 6...\n",
      "Extracting Числовые значения имен и фамилий 7...\n",
      "Extracting Числовые значения имен и фамилий 8...\n",
      "Extracting Числовые значения имен и фамилий 9...\n",
      "Extracting Числовые значения имен и фамилий 10...\n",
      "Extracting Числовые значения имен и фамилий 11...\n",
      "Extracting Числовые значения имен и фамилий 12...\n",
      "Extracting Числовые значения имен и фамилий 13...\n",
      "Extracting Числовые значения имен и фамилий 14...\n",
      "Extracting Числовые значения имен и фамилий 15...\n",
      "Extracting Числовые значения имен и фамилий 16...\n",
      "Extracting Числовые значения имен и фамилий 17...\n",
      "Extracting Числовые значения имен и фамилий 18...\n",
      "Extracting Числовые значения имен и фамилий 19...\n",
      "Extracting Числовые значения имен и фамилий 20...\n",
      "Extracting Числовые значения имен и фамилий 21...\n",
      "Extracting Числовые значения имен и фамилий 22...\n",
      "Dictionary saved to output/Числовые значения имен и фамилий.json\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "\n",
    "class TranscriptProcessor:\n",
    "    def __init__(self):\n",
    "        self.transcripts_dir = os.getenv(\"TRANSCRIPTS_PATH\")\n",
    "        if not self.transcripts_dir:\n",
    "            raise ValueError(\"TRANSCRIPTS_PATH environment variable is not set.\")\n",
    "        self.extractor = LectureKnowledgeExtractor()\n",
    "        self.lectures = self.get_lectures_structure()\n",
    "\n",
    "    def get_lectures_structure(self):\n",
    "        lectures = {}\n",
    "        input_files = glob.glob(os.path.join(self.transcripts_dir, \"*.txt\"))\n",
    "        for path_to_doc in input_files:\n",
    "            lecture = self.extractor.extract_doc_structure(path_to_doc)\n",
    "            if \"topic\" in lecture and \"variations\" in lecture:\n",
    "                lectures[lecture[\"topic\"]] = {\"path_to_doc\": path_to_doc, \"variations\": lecture[\"variations\"]}\n",
    "            else:\n",
    "                raise ValueError(f\"Failure during extracting a structure of lecture {path_to_doc}. Unexpected structure.\")\n",
    "        return lectures\n",
    "\n",
    "    @staticmethod\n",
    "    def save_to_json(data_dict, filename):    \n",
    "        # Ensure output directory exists\n",
    "        output_dir = 'output'\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Create full file path\n",
    "        file_path = os.path.join(output_dir, filename)\n",
    "        \n",
    "        # Save dictionary to JSON file\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data_dict, f, indent=4, ensure_ascii=False)\n",
    "        print(f\"Dictionary saved to {file_path}\")\n",
    "\n",
    "    def process_lectures(self):\n",
    "        for lecture in self.lectures:\n",
    "            lecture_knowledge = self.extractor.extract_lecture_knowledge(lecture, lectures[lecture][\"variations\"], lectures[lecture][\"path_to_doc\"])\n",
    "            self.save_to_json({lecture: lecture_knowledge}, f\"{lecture}.json\")\n",
    "            break\n",
    "\n",
    "processor = TranscriptProcessor()\n",
    "processor.process_lectures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_interpretation_stuct_prompt(chapter_name, variation_list, output):\n",
    "    prompt = f\"\"\"Это json, содержащий текстовые харакетриситики {len(variation_list)} вариантов {chapter_name}: {output}. \n",
    "    Я бы хотел его переформатировать таким образом, чтобы описание каждого из 12 {chapter_name} использовало единообразную структуру. \n",
    "    Сгенерируй список из 4 заголовков, которые содержали бы не больше одного уровня иерархии и которые можно было бы использовать для всех {len(variation_list)} вариантов {chapter_name}. \n",
    "    Не изменяй исходный текст. Ответ должн содержать только новые заголовки в формате json. Например, {\"text\": [\"подзаголовок1\", \"подзаголовок2\",\"подзаголовок3\", \"подзаголовок4\"]}.\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
